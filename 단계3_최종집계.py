# -*- coding: utf-8 -*-
"""
ìµœì¢… ë°ì´í„° ì§‘ê³„ ìµœì í™” ë²„ì „ (ë‹¨ê³„3)
============================
ê¸°ëŠ¥: ìŠˆí¼ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹í—ˆ ë°ì´í„°ë¥¼ final_outcome íŒŒì¼ì— ì§‘ê³„

ê°œì„ ì :
1. ìƒì„¸í•œ ë¡œê·¸ ê¸°ë¡
2. ì§„í–‰ ìƒí™© í‘œì‹œ
3. ë°ì´í„° ê²€ì¦
4. ë²¡í„°í™”ëœ ë°œëª…ì í†µê³„ (í•œêµ­ì–´ ìš”êµ¬ì‚¬í•­)
5. ëˆ„ë½ëœ ì»¬ëŸ¼ ìë™ ì²˜ë¦¬
"""

import pandas as pd
import numpy as np
import pickle
import logging
from datetime import datetime
from tqdm import tqdm

# ==========================================
# ë¡œê·¸ ì„¤ì •
# ==========================================
# logs í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
import os
if not os.path.exists('logs'):
    os.makedirs('logs')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/aggregation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# 1. ê²½ë¡œ ì„¤ì •
# ==========================================
# ìŠˆí¼ ì‚¬ì „ ê²½ë¡œ
DICT_PATH = '/Users/lidachuan/Desktop/Patent Data/Master_Company_Dictionary.pkl'

# ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ í…œí”Œë¦¿ ê²½ë¡œ
FINAL_OUTCOME_PATH = '/Users/lidachuan/Desktop/Patent Data/final_outcome.xlsx'

# íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ (ë‹¨ì¼ CSV ë˜ëŠ” ë””ë ‰í† ë¦¬)
PATENT_DB_PATH = '/Users/lidachuan/Desktop/Patent Data/1993-1997/patent_database.csv'

# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
OUTPUT_PATH = '/Users/lidachuan/Desktop/Patent Data/final_outcome_1993_1997_COMPLETE.xlsx'

# ==========================================
# 2. ë°œëª…ì í†µê³„ í•¨ìˆ˜ (í•œêµ­ì–´ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜)
# ==========================================

def calculate_inventor_count_vectorized(df, inventor_cols):
    """
    í•œêµ­ì–´ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼: inventors ì»¬ëŸ¼ê³¼ ì´ë¦„ ì»¬ëŸ¼ ê°œìˆ˜ ì¤‘ ë” í° ê°’ ì‚¬ìš©
    ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    """
    # 1. inventors ì»¬ëŸ¼ì—ì„œ ìˆ«ì ê°€ì ¸ì˜¤ê¸°
    num_from_column = pd.to_numeric(df['inventors'], errors='coerce').fillna(0)
    
    # 2. ì´ë¦„ ì»¬ëŸ¼ì—ì„œ ê°œìˆ˜ ì„¸ê¸° (ë²¡í„°í™”ëœ ì—°ì‚°)
    num_from_names = df[inventor_cols].notna().sum(axis=1)
    
    # 3. ë‘ ê°’ ì¤‘ ë” í° ê°’ ì‚¬ìš© (í•œêµ­ì–´ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜)
    return np.maximum(num_from_column, num_from_names)

# ==========================================
# 3. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
# ==========================================

def load_master_dictionary():
    """ìŠˆí¼ ì‚¬ì „ ë¡œë“œ"""
    logger.info("ë‹¨ê³„ 1/6: ìŠˆí¼ ì‚¬ì „ ë¡œë“œ ì¤‘...")
    try:
        with open(DICT_PATH, 'rb') as f:
            master_dict = pickle.load(f)
        logger.info(f"   âœ… ì‚¬ì „ ë¡œë“œ ì„±ê³µ, {len(master_dict):,} ê°œ ë§¤í•‘ ê´€ê³„ í¬í•¨")
        return master_dict
    except FileNotFoundError:
        logger.error(f"   âŒ ì˜¤ë¥˜: ì‚¬ì „ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ {DICT_PATH}")
        logger.error("   ë¨¼ì € ë‹¨ê³„2(ìŠˆí¼ ì‚¬ì „ êµ¬ì¶•)ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        raise


def load_main_database():
    """ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ í…œí”Œë¦¿ ë¡œë“œ"""
    logger.info("\në‹¨ê³„ 2/6: ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ í…œí”Œë¦¿ ë¡œë“œ ì¤‘...")
    try:
        df_main = pd.read_excel(FINAL_OUTCOME_PATH)
        df_main.drop_duplicates(subset=['acquiror_name'], keep='first', inplace=True)
        logger.info(f"   âœ… í…œí”Œë¦¿ ë¡œë“œ ì„±ê³µ, ì´ {len(df_main):,} ê°œ íšŒì‚¬")
        return df_main
    except FileNotFoundError:
        logger.error(f"   âŒ ì˜¤ë¥˜: í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ {FINAL_OUTCOME_PATH}")
        raise


def load_patent_database():
    """íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    logger.info("\në‹¨ê³„ 3/6: íŠ¹í—ˆ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
    try:
        df_patent = pd.read_csv(PATENT_DB_PATH, low_memory=False)
        original_count = len(df_patent)
        
        # assigneeê°€ ë¹„ì–´ìˆëŠ” í–‰ ì œê±°
        df_patent.dropna(subset=['assignee'], inplace=True)
        logger.info(f"   âœ… íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_patent):,} ê±´ ìœ íš¨ ë ˆì½”ë“œ (ì›ë³¸ {original_count:,})")
        return df_patent
    except FileNotFoundError:
        logger.error(f"   âŒ ì˜¤ë¥˜: íŠ¹í—ˆ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ {PATENT_DB_PATH}")
        raise


def process_patent_data(df_patent, master_dict):
    """íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬: ì‚¬ì „ ë§¤í•‘ ì ìš© ë° ë°œëª…ì í†µê³„"""
    logger.info("\në‹¨ê³„ 4/6: íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    
    # ë§¤í•‘ ì ìš©
    logger.info("   ì‚¬ì „ ë§¤í•‘ ì ìš© ì¤‘...")
    df_patent['assignee_stripped'] = df_patent['assignee'].astype(str).str.strip()
    df_patent['Matched_Acquiror'] = df_patent['assignee_stripped'].map(master_dict)
    
    # ë§¤ì¹­ë¥  í†µê³„
    matched_count = df_patent['Matched_Acquiror'].notna().sum()
    match_rate = matched_count / len(df_patent) * 100
    logger.info(f"   âœ… ë§¤í•‘ ì™„ë£Œ: {matched_count:,} / {len(df_patent):,} ({match_rate:.2f}%)")
    
    # ë§¤ì¹­ ì„±ê³µí•œ ê²ƒë§Œ ìœ ì§€
    df_matched = df_patent.dropna(subset=['Matched_Acquiror']).copy()
    
    # ì—°ë„ ì •ë¦¬
    logger.info("   ì—°ë„ ë°ì´í„° ì •ë¦¬ ì¤‘...")
    df_matched['application_year'] = pd.to_numeric(df_matched['application_year'], errors='coerce')
    df_matched = df_matched.dropna(subset=['application_year'])
    df_matched['application_year'] = df_matched['application_year'].astype(int)
    
    # ë°œëª…ì ìˆ˜ í†µê³„ (í•œêµ­ì–´ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜)
    logger.info("   ë°œëª…ì ìˆ˜ ê³„ì‚° ì¤‘...")
    inventor_name_cols = [f'inventor_name{i}' for i in range(1, 11)]
    
    # ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    for col in inventor_name_cols:
        if col not in df_matched.columns:
            df_matched[col] = np.nan
    
    df_matched['final_inventor_count'] = calculate_inventor_count_vectorized(
        df_matched, 
        inventor_name_cols
    )
    
    logger.info(f"   âœ… ì²˜ë¦¬ ì™„ë£Œ, íŠ¹í—ˆ ë‹¹ í‰ê·  {df_matched['final_inventor_count'].mean():.2f} ëª… ë°œëª…ì")
    
    return df_matched


def aggregate_data(df_matched):
    """ë°ì´í„° ì§‘ê³„: íšŒì‚¬ ë° ì—°ë„ë³„ í†µê³„"""
    logger.info("\në‹¨ê³„ 5/6: ë°ì´í„° ì§‘ê³„ ì¤‘...")
    
    # íšŒì‚¬ ë° ì—°ë„ë³„ ê·¸ë£¹í™”
    logger.info("   íšŒì‚¬ ë° ì—°ë„ë³„ ê·¸ë£¹ í†µê³„ ì¤‘...")
    df_grouped = df_matched.groupby(['Matched_Acquiror', 'application_year']).agg({
        'assignee': 'count',  # íŠ¹í—ˆ ìˆ˜
        'final_inventor_count': 'sum'  # ë°œëª…ì ì´ìˆ˜
    }).reset_index()
    
    # í”¼ë²— í…Œì´ë¸”: íŠ¹í—ˆ ìˆ˜
    logger.info("   íŠ¹í—ˆ ìˆ˜ í”¼ë²— í…Œì´ë¸” ìƒì„± ì¤‘...")
    pivot_patent = df_grouped.pivot(
        index='Matched_Acquiror', 
        columns='application_year', 
        values='assignee'
    )
    pivot_patent.columns = [f'patent_{int(col)}' for col in pivot_patent.columns]
    
    # í”¼ë²— í…Œì´ë¸”: ë°œëª…ì ìˆ˜
    logger.info("   ë°œëª…ì ìˆ˜ í”¼ë²— í…Œì´ë¸” ìƒì„± ì¤‘...")
    pivot_inventor = df_grouped.pivot(
        index='Matched_Acquiror', 
        columns='application_year', 
        values='final_inventor_count'
    )
    pivot_inventor.columns = [f'patent_inventor_{int(col)}' for col in pivot_inventor.columns]
    
    # í”¼ë²— í…Œì´ë¸” ë³‘í•©
    df_stats = pd.concat([pivot_patent, pivot_inventor], axis=1).reset_index()
    df_stats.rename(columns={'Matched_Acquiror': 'acquiror_name'}, inplace=True)
    
    logger.info(f"   âœ… ì§‘ê³„ ì™„ë£Œ, {len(df_stats)} ê°œ íšŒì‚¬ í¬í•¨")
    logger.info(f"   ì—°ë„ ë²”ìœ„: {pivot_patent.columns.tolist()[:3]}...{pivot_patent.columns.tolist()[-3:]}")
    
    # íšŒì‚¬ ë³„ì¹­ ìˆ˜ì§‘
    logger.info("   íšŒì‚¬ ë³„ì¹­ ìˆ˜ì§‘ ì¤‘...")
    df_names = df_matched.groupby('Matched_Acquiror')['assignee'].apply(
        lambda x: list(set(x))
    ).reset_index()
    
    # ë³„ì¹­ ëª©ë¡ í™•ì¥
    max_len = df_names['assignee'].apply(len).max() if not df_names.empty else 0
    name_cols = ['patent_name'] + [f'patent_name_{i}' for i in range(1, max_len)]
    names_expanded = pd.DataFrame(df_names['assignee'].tolist(), index=df_names.index)
    names_expanded = names_expanded.iloc[:, :len(name_cols)]
    names_expanded.columns = name_cols[:names_expanded.shape[1]]
    
    df_names = pd.concat([df_names[['Matched_Acquiror']], names_expanded], axis=1)
    df_names.rename(columns={'Matched_Acquiror': 'acquiror_name'}, inplace=True)
    
    return df_stats, df_names


def merge_to_final_outcome(df_main, df_stats, df_names):
    """ìµœì¢… íŒŒì¼ì— ë³‘í•©"""
    logger.info("\në‹¨ê³„ 6/6: ìµœì¢… íŒŒì¼ ë³‘í•© ì¤‘...")
    
    # ê¸°ì¡´ì˜ ì˜¤ë˜ëœ ì»¬ëŸ¼ ì •ë¦¬
    logger.info("   ê¸°ì¡´ í†µê³„ ì»¬ëŸ¼ ì •ë¦¬ ì¤‘...")
    cols_to_remove = [c for c in df_main.columns 
                     if c.startswith('patent_') or c.startswith('patent_inventor_')]
    if cols_to_remove:
        df_main = df_main.drop(columns=cols_to_remove, errors='ignore')
        logger.info(f"   {len(cols_to_remove)} ê°œ ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°í•¨")
    
    # í†µê³„ ë°ì´í„° ë³‘í•©
    logger.info("   í†µê³„ ë°ì´í„° ë³‘í•© ì¤‘...")
    df_final = pd.merge(df_main, df_stats, on='acquiror_name', how='left')
    
    # ë³„ì¹­ ë°ì´í„° ë³‘í•©
    logger.info("   ë³„ì¹­ ë°ì´í„° ë³‘í•© ì¤‘...")
    df_final = pd.merge(df_final, df_names, on='acquiror_name', how='left')
    
    # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ìˆ«ì ì»¬ëŸ¼ë§Œ)
    stat_cols = [c for c in df_final.columns 
                if (c.startswith('patent_') or c.startswith('patent_inventor_')) 
                and 'name' not in c]
    df_final[stat_cols] = df_final[stat_cols].fillna(0).astype(int)
    
    logger.info(f"   âœ… ë³‘í•© ì™„ë£Œ, ìµœì¢… íŒŒì¼ ì´ {len(df_final)} í–‰")
    
    # ë°ì´í„°ê°€ ìˆëŠ” íšŒì‚¬ í†µê³„
    companies_with_patents = (df_final[stat_cols].sum(axis=1) > 0).sum()
    logger.info(f"   ê·¸ ì¤‘ {companies_with_patents} ê°œ íšŒì‚¬ì— íŠ¹í—ˆ ë°ì´í„° ìˆìŒ")
    
    return df_final


def save_output(df_final):
    """ì¶œë ¥ íŒŒì¼ ì €ì¥"""
    logger.info("\nê²°ê³¼ ì €ì¥ ì¤‘...")
    df_final.to_excel(OUTPUT_PATH, index=False)
    logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")


# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
# ==========================================

def main():
    start_time = datetime.now()
    
    logger.info("=" * 60)
    logger.info("ìµœì¢… ë°ì´í„° ì§‘ê³„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ìµœì í™” ë²„ì „)")
    logger.info("=" * 60)
    
    try:
        # ë‹¨ê³„1: ì‚¬ì „ ë¡œë“œ
        master_dict = load_master_dictionary()
        
        # ë‹¨ê³„2: ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        df_main = load_main_database()
        
        # ë‹¨ê³„3: íŠ¹í—ˆ ë°ì´í„° ë¡œë“œ
        df_patent = load_patent_database()
        
        # ë‹¨ê³„4: íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬
        df_matched = process_patent_data(df_patent, master_dict)
        
        # ë‹¨ê³„5: ë°ì´í„° ì§‘ê³„
        df_stats, df_names = aggregate_data(df_matched)
        
        # ë‹¨ê³„6: ìµœì¢… íŒŒì¼ì— ë³‘í•©
        df_final = merge_to_final_outcome(df_main, df_stats, df_names)
        
        # ê²°ê³¼ ì €ì¥
        save_output(df_final)
        
        # ì™„ë£Œ ìš”ì•½
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("\n" + "=" * 60)
        logger.info("ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info("=" * 60)
        logger.info(f"â±  ì´ ì†Œìš”ì‹œê°„: {duration:.2f} ì´ˆ")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(df_patent) / duration:.0f} ê±´/ì´ˆ")
        logger.info(f"\nâœ… ë‹¤ìŒ ë‹¨ê³„: ë‹¨ê³„4(Compustat ë§¤ì¹­) ì‹¤í–‰")
        
        return True
        
    except Exception as e:
        logger.error(f"\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
